Q5. Use Naive bayes, K-nearest, and Decision tree classification algorithms and build classifiers.
Divide the data set into training and test set. Compare the accuracy of the different classifiers
under the following situations:
5.1 a) Training set = 75% Test set = 25% b) Training set = 66.6% (2/3rd of total), Test set =
33.3%
5.2 Training set is chosen by i) hold out method ii) Random subsampling iii) Cross-Validation.
Compare the accuracy of the classifiers obtained.
5.3 Data is scaled to standard format.





from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Function to print accuracy, confusion matrix, classification report, and decision tree plot
def print_results(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy:", accuracy)
    
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:\n", cm)
    
    report = classification_report(y_test, y_pred)
    print("Classification Report:\n", report)
    
    if isinstance(model, DecisionTreeClassifier):
        plt.figure(figsize=(8, 6))
        plot_tree(model, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)
        plt.show()

# Scenario implementation
def run_scenario(test_size, subsampling=False):
    if subsampling:
        indices = np.random.choice(len(X), size=int(len(X) * test_size), replace=False)
        X_subsample = X[indices]
        y_subsample = y[indices]
        X_train, X_test, y_train, y_test = train_test_split(X_subsample, y_subsample, test_size=test_size, random_state=42)
    else:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Decision Tree Classifier
    print("Decision Tree Classifier:")
    dt = DecisionTreeClassifier(criterion='entropy')
    print_results(dt, X_train, X_test, y_train, y_test)

    # K-Nearest Neighbors Classifier
    print("K-Nearest Neighbors Classifier:")
    knn = KNeighborsClassifier()
    print_results(knn, X_train, X_test, y_train, y_test)

    # Naive Bayes Classifier
    print("Naive Bayes Classifier:")
    nb = GaussianNB()
    print_results(nb, X_train, X_test, y_train, y_test)

if __name__ == "__main__":
    # Holdout Method with 25% test size
    print("\nHoldout Method with 25% Test Size:")
    run_scenario(test_size=0.25)

    # Holdout Method with 33% test size
    print("\nHoldout Method with 33% Test Size:")
    run_scenario(test_size=0.33)

    # Random Subsampling Method with 25% test size
    print("\nRandom Subsampling Method with 25% Test Size:")
    run_scenario(test_size=0.25, subsampling=True)

    # Random Subsampling Method with 33% test size
    print("\nRandom Subsampling Method with 33% Test Size:")
    run_scenario(test_size=0.33, subsampling=True)

    # Cross-Validation Method with 25% test size
    print("\nCross-Validation Method with 25% Test Size:")
    cv_accuracy_25 = cross_val_score(DecisionTreeClassifier(criterion='entropy'), X, y, cv=5)
    print("Mean Accuracy (Decision Tree) using Cross-Validation:", np.mean(cv_accuracy_25))

    cv_accuracy_25 = cross_val_score(KNeighborsClassifier(), X, y, cv=5)
    print("Mean Accuracy (K-Nearest Neighbors) using Cross-Validation:", np.mean(cv_accuracy_25))

    cv_accuracy_25 = cross_val_score(GaussianNB(), X, y, cv=5)
    print("Mean Accuracy (Naive Bayes) using Cross-Validation:", np.mean(cv_accuracy_25))

    # Cross-Validation Method with 33% test size
    print("\nCross-Validation Method with 33% Test Size:")
    cv_accuracy_33 = cross_val_score(DecisionTreeClassifier(criterion='entropy'), X, y, cv=5)
    print("Mean Accuracy (Decision Tree) using Cross-Validation:", np.mean(cv_accuracy_33))

    cv_accuracy_33 = cross_val_score(KNeighborsClassifier(), X, y, cv=5)
    print("Mean Accuracy (K-Nearest Neighbors) using Cross-Validation:", np.mean(cv_accuracy_33))

    cv_accuracy_33 = cross_val_score(GaussianNB(), X, y, cv=5)
    print("Mean Accuracy (Naive Bayes) using Cross-Validation:", np.mean(cv_accuracy_33))

    # Scaling data using StandardScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Holdout Method with 25% test size (Scaled Data)
    print("\nHoldout Method with 25% Test Size (Scaled Data):")
    run_scenario(test_size=0.25, subsampling=False)

    # Holdout Method with 33% test size (Scaled Data)
    print("\nHoldout Method with 33% Test Size (Scaled Data):")
    run_scenario(test_size=0.33, subsampling=False)
